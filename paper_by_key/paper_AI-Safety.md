# Papers with Keyword: AI-Safety

- [Qwen3Guard Technical Report](https://arxiv.org/pdf/2510.14276)
    -  Qwen Team
    - ğŸ›ï¸ Institutions:  Qwen Team
    - ğŸ“… Date: Oct. 16, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Misc]
    - ğŸ”‘ Key: [benchmark], [evaluation], [AI-Safety], [Model]
    - ğŸ“– TLDR: Existing LLM safety classifiers are limited by binary labels and delayed detection. We introduce Qwen3Guard, a multilingual safety model series featuring: (1) Generative Qwen3Guard for fineâ€‘grained triâ€‘class classification (safe/controversial/unsafe). (2) Stream Qwen3Guard for realâ€‘time, tokenâ€‘level monitoring during streaming generation.

- [AEGIS2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails](https://arxiv.org/pdf/2501.09004)
    -  Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, Christopher Parisien
    - ğŸ›ï¸ Institutions:  Nvidia
    - ğŸ“… Date: Jan. 15, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Misc]
    - ğŸ”‘ Key: [dataset], [evaluation], [AI-Safety]
    - ğŸ“– TLDR: To address the lack of high-quality, commercially usable safety datasets for LLMs, we created Aegis 2.0. This dataset features a novel, fine-grained taxonomy (12 main hazard categories, 9 subcategories) for classifying risks. It contains over 34,000 human-LLM interactions, annotated via a hybrid human+LLM jury pipeline. Crucially, lightweight models fine-tuned on Aegis 2.0 match the performance of models trained on much larger, non-commercial datasets. We also introduce a safety+topic training blend that improves model adaptability to new risks. All data and models will be open-sourced to advance LLM safety research.

- [Introducing v0.5 of the AI Safety Benchmark from MLCommons](https://arxiv.org/abs/2404.12241)
    -  MLCommons AI Safety Working Group (WG)
    - ğŸ›ï¸ Institutions:  MLCommons AI Safety Working Group (WG), et al.
    - ğŸ“… Date: May. 13, 2024
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Misc]
    - ğŸ”‘ Key: [benchmark], [evaluation], [AI-Safety], [AI-systems]
    - ğŸ“– TLDR: This paper presents v0.5 of the AI Safety Benchmark by the MLCommons AI Safety Working Group, designed to evaluate safety risks of chat-tuned AI systems. Version 0.5 covers a single English chat use case with typical, malicious, and vulnerable personas. It introduces 13 hazard categories, with tests for seven, totaling 43,090 prompts. The release includes benchmark specifications, a grading system, the ModelBench evaluation platform, an example report, and documentation of limitations. Version 1.0, planned for late 2024, will offer broader safety insights.
